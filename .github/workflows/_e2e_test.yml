name: 'e2e-test'

on:
  workflow_call:
    inputs:
      vllm:
        required: true
        type: string
      runner:
        required: true
        type: string
      image:
        required: true
        type: string
      type:
        required: true
        type: string
      vllm-kunlun:
        required: true
        type: string


jobs:
  e2e:
    name: singlecard
    runs-on: 
      - self-hosted
      - Linux
      - X64
    steps:

      - name: create docker
        run: |
          sudo bash /workspace/start_docker.sh create
          sudo docker exec aiak-hcc-111 bash -lc "
            echo 'conda activate python310_torch25_cuda' >> ~/.bashrc
          " 

      - name : Install vLLM 0.11.0
        run: |
          sudo docker exec aiak-hcc-111 bash -lc "
            #source /root/miniconda/etc/profile.d/conda.sh
            conda activate python310_torch25_cuda
            conda env list
            pip uninstall -y vllm
            env | grep -i proxy
            pip install vllm==0.11.0 --no-build-isolation --no-deps --index-url https://pip.baidu-int.com/simple/
          "
      - name : Install vLLM-kunlun 
        run: |
          sudo docker exec aiak-hcc-111 bash -lc "
            cd /workspace
            export {http,https}_proxy=http://agent.baidu.com:8891
            git clone https://github.com/baidu/vLLM-Kunlun
            unset http_proxy&&unset https_proxy
            cd vLLM-Kunlun
            pip install -r requirements.txt
            python setup.py build
            python setup.py install
            cp vllm_kunlun/patches/eval_frame.py /root/miniconda/envs/python310_torch25_cuda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py
            wget -O xpytorch-cp310-torch251-ubuntu2004-x64.run https://baidu-kunlun-public.su.bcebos.com/v1/baidu-kunlun-share/1130/xpytorch-cp310-torch251-ubuntu2004-x64.run?authorization=bce-auth-v1%2FALTAKypXxBzU7gg4Mk4K4c6OYR%2F2025-12-02T05%3A01%3A27Z%2F-1%2Fhost%2Ff3cf499234f82303891aed2bcb0628918e379a21e841a3fac6bd94afef491ff7
            bash xpytorch-cp310-torch251-ubuntu2004-x64.run
            pip install "https://baidu-kunlun-public.su.bcebos.com/v1/baidu-kunlun-share/1130/xtorch_ops-0.1.2209%2B6752ad20-cp310-cp310-linux_x86_64.whl?authorization=bce-auth-v1%2FALTAKypXxBzU7gg4Mk4K4c6OYR%2F2025-12-05T06%3A18%3A00Z%2F-1%2Fhost%2F14936c2b7e7c557c1400e4c467c79f7a9217374a7aa4a046711ac4d948f460cd"
            pip install "https://cce-ai-models.bj.bcebos.com/v1/vllm-kunlun-0.11.0/triton-3.0.0%2Bb2cde523-cp310-cp310-linux_x86_64.whl"
            pip install "https://cce-ai-models.bj.bcebos.com/XSpeedGate-whl/release_merge/20251219_152418/xspeedgate_ops-0.0.0-cp310-cp310-linux_x86_64.whl"
            chmod +x /workspace/vLLM-Kunlun/setup_env.sh && source /workspace/vLLM-Kunlun/setup_env.sh
          "

      - name: Install evalscope && evalscope[perf]
        run: |
            sudo docker exec  aiak-hcc-111 bash -lc '
              export {http,https}_proxy=http://10.162.37.16:8128
              export NO_PROXY=localhost,127.0.0.1,::1 && export no_proxy=localhost,127.0.0.1,::1
              pip install evalscope
              pip install 'evalscope[perf]'
            '
      - name: Module import & version check
        run: |
          sudo docker exec aiak-hcc-111 bash -lc '
            conda activate python310_torch25_cuda
            echo "=== vLLM Version ==="
            python -c "import vllm; print(vllm.__version__)"
            echo "=== vLLM-Kunlun Version ==="
            python -c "import importlib.metadata as m; print(m.version('vllm_kunlun'))"
            echo "=== evalscope ==="
            pip show evalscope
          '
      - name: start server
        run: |
          sudo docker exec -d aiak-hcc-111 bash -lc '
            chmod +x /workspace/vLLM-Kunlun/setup_env.sh && source /workspace/vLLM-Kunlun/setup_env.sh
            export XPU_VISIBLE_DEVICES=6
            python -m vllm.entrypoints.openai.api_server \
              --host 0.0.0.0 \
              --port 8356 \
              --model /ssd1/models/Qwen3-14B \
              --gpu-memory-utilization 0.9 \
              --trust-remote-code \
              --max-model-len 32768 \
              --tensor-parallel-size 1 \
              --dtype float16 \
              --max_num_seqs 128 \
              --max_num_batched_tokens 32768 \
              --block-size 128 \
              --no-enable-prefix-caching \
              --no-enable-chunked-prefill \
              --distributed-executor-backend mp \
              --served-model-name Qwen3-14B \
              --compilation-config '\''{"splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output","vllm.unified_attention_with_output_kunlun","vllm.mamba_mixer2", "vllm.mamba_mixer", "vllm.short_conv", "vllm.linear_attention", "vllm.plamo2_mamba_mixer", "vllm.gdn_attention", "vllm.sparse_attn_indexer"]}'\''
              > /workspace/vllm.log 2>&1
              '
      - name: wait for vllm
        run: |
          sudo docker exec aiak-hcc-111 bash -lc '
            echo "Waiting for vLLM..."
              for i in {1..60}; do
                if curl -sf http://127.0.0.1:8356/v1/models >/dev/null; then
                  echo "vLLM is ready"
                  tail -n 200 /workspace/vllm.log || true
                  break
                fi
                sleep 5
              done
              if ! curl -sf http://127.0.0.1:8356/v1/models >/dev/null; then
                echo "vLLM start failed"
                echo "==== last 200 lines of vllm.log ===="
                tail -n 200 /workspace/vllm.log || true
                exit 1
              fi
          '
      - name: Accuracy testing
        run: |
          sudo docker exec aiak-hcc-111 bash -lc '
            export {http,https}_proxy=http://10.162.37.16:8128
            export NO_PROXY=localhost,127.0.0.1,::1 && export no_proxy=localhost,127.0.0.1,::1
            evalscope eval \
              --model Qwen3-14B \
              --api-url http://localhost:8356/v1 \
              --datasets gsm8k arc \
              --limit 10
              --output /workspace/evalscope_accuracy_report.json
          '
      

      - name: Performerance testing
        run: |
          sudo docker exec aiak-hcc-111 bash -lc '
            export {http,https}_proxy=http://10.162.37.16:8128
            export NO_PROXY=localhost,127.0.0.1,::1 && export no_proxy=localhost,127.0.0.1,::1
            evalscope perf \
              --model Qwen3-14B \
              --url "http://localhost:8356/v1/chat/completions" \
              --parallel 5 \
              --number 20 \
              --api openai \
              --dataset openqa \
              --stream
              --output /workspace/evalscope_performance_report.json
          '
      - name: Upload evalscope reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evalscope-reports-${{ github.run_id }}
          path: |
            /workspace/evalscope_accuracy_report.json
            /workspace/evalscope_performance_report.json

      - name: Upload vLLM log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: vllm-log-${{ github.run_id }}
          path: /workspace/vllm.log
      - name: Cleanup docker
        run: |
          sudo docker stop aiak-hcc-111 || true
          sudo docker rm aiak-hcc-111 || true
      